---
title: Performance Metrics
---

::: callout-important
#### text section & script

[@deitel_intro_2019] Section 15.3.1 Metrics for Model Accuracy

Study both [the text's Jupyter Notebook](https://github.com/pdeitel/IntroToPython/blob/master/examples/ch15/snippets_ipynb/15_02-03.ipynb) and the code on this page.
:::

## Metrics for Model Scoring

We use the following confusion matrix to learn a few performance metrics.

|              |   prediction   |               |
|-------------:|:--------------:|:-------------:|
|        truth | *0 / negative* | *1 /positive* |
| 0 / negative |     TN = 1     |    FP = 2     |
| 1 / positive |     FN = 3     |    TP = 4     |

: An example of confusion matrix

::: callout-important
### QUESTION: Defining evaluation metrics for classifier evaluation

Define the following metrics for performance evaluation of classifiers: Accuracy, Precision, Recall, Sensitivity, Specificity, $F_1-score$, $F_\beta-score$
:::

### Accuracy

We define **overall success rate** (or accuracy) as a metric defining what the classifier has correctly done.

-   **The ratio** between the sum of the diagonal values (i.e., TP and TN) vs. the sum of the matrix.

$$
accuracy = \frac{TP+TN}{TP+TN+FP+FN} = \frac{4+1}{10}= 0.5
$$

In other words, the confusion matrix of a good model has large numbers diagonally and small (ideally zero) numbers off-diagonally.

### Precision, Recall, Sensitivity, Specificity

`Precision` and `Recall` are the two *accuracy metrics* used in the **information retrieval** community; they are often used to characterize classifiers as well.

::: panel-tabset
#### Precision

**Precision** is the percent of cases we (the model) marked `positive` really are positive.

This tells us how **reliable** it is when a classifier has marked a case as positive.

$$
precision = \frac{TP}{TP+FP} = \frac{4}{4+2} \approx 0.667
$$ In a classification report, the precision specifies the total number of correct predictions for a class divided by the total number of that class in the predictions.

#### Recall & Sensitivity

**Recall** is the percent of `positive` cases we were able to correctly identify and select. Recall is also called **Sensitivity** and **True Positive Rate**, the portion of *true positive cases* that a classifier is able to discover.

$$
recall = TPR = \frac{TP}{TP+FN} = \frac{4}{4+3}\approx 0.571
$$ In a classification report, the recall specifies the total number of correct predictions for a class divided by *the total number of that class in the truth*.

#### Specificity

**Specificity**, also called **True Negative Rate**, is a metric to evaluate how good a classifier `identifies negative class`, calculated as the portion of the negative cases which has identified by a classifier.

$$
specificity = TNR = \frac{TN}{TN+FP} = \frac{1}{1+2} \approx 0.333
$$

The **false positive rate**, FPR, is $$
FPR = \frac{FP}{TN+FP} = 1-specificity
$$
:::

::: callout-important
## QUESTION: Reading Confusion Matrix

Given a confusion matrix having the truth by columns and the predictions by rows. The matrix is used to evaluate a 10-class classifier.

    [[45  0  0  0  0  0  0  0  0  0]
     [ 0 45  0  0  0  0  0  0  0  0]
     [ 0  0 54  0  0  0  0  0  0  0]
     [ 0  0  0 42  0  1  0  1  0  0]
     [ 0  0  0  0 49  0  0  1  0  0]
     [ 0  0  0  0  0 38  0  0  0  0]
     [ 0  0  0  0  0  0 42  0  0  0]
     [ 0  0  0  0  0  0  0 45  0  0]
     [ 0  1  1  2  0  0  0  0 39  1]
     [ 0  0  0  0  1  0  0  0  1 41]]

Explain row 9 of the confusion matrix. And what is the precision and recall of the classifier for the 9th class?
:::

### Balanced Accuracy

Accuracy is used to evaluate how good a binary classifier is. When the two classes are **imbalanced**, i.e., one class appears more frequently than the other, the balanced accuracy should be taken as the average of the `sensitivity score for positive` class and the `specificity score for negative class`. $$
\text{balanced accuracy} = \frac{sensitivity+specificity}{2} \approx 0.452
$$





### $F_1$-score

In a binary classification, `F-score` or `F-measure` is calculated from the **precision** and **recall** of the test. The traditional F-score, $F_1-score$, is the average of precision and recall.

The highest possible value of an $F_1$-score is 1 and the lowest is 0.

$$
F_1\text{} = \frac{precision+recall}{2} \approx 0.615
$$


::: column-margin

In his 1979 book Information Retrieval, Cornelis Joost van Rijsbergen defined a function very similar to the F-score, recognizing the inadequacy of accuracy as a metric for information retrieval systems. He called the metric the Effectiveness function, and assigned it the letter E, because it measures the _effectiveness of retrieval with respect to a user who attaches Î² times as much importance to recall as precision_. It is not known why the F-score is assigned the letter F today.
:::


### $F_\beta$-score

A more general F-score, $F_\beta$, uses a positive factor $\beta$ by which the score will weigh recall $\beta$ times as important as precision.

Recall measures how sensitive a model is to the positive class. Precision is about how many cases predicted as positive are really positive.

::: callout-tip
#### Choosing the $\beta$ value for $F_\beta$-score

If discovering all the positive cases is more important than filtering false positive cases, then $\beta$ should be greater than $1$.
:::

Two commonly used values for $\beta$ are $2$, which weighs recall higher than precision, and $0.5$, which weighs recall lower than precision.

$$
F_\beta\text{} = (1+\beta^2)\times\frac{precision\times recall}{\beta^2\times precision + recall} 
$$

::: column-margin
A loss or cost function is a technique which is used to represent the loss or cost of an event, such as mistakenly classifying a negative case as positive or vice verse.
:::

::: callout-important
## QUESTION: Evaluating a two-class classifier

Assume a binary classifier using `yes` as positive class and `no` as negative class. Given the following truth and prediction data.

    truth: "yes" "yes" "yes" "no"  "no"  "no"  "no"  "no"  "no"  "yes"
    prediction: "no"  "yes" "no"  "yes" "yes" "no"  "no"  "yes" "yes" "no"

1.  Calculate precision, recall/sensitivity, specificity, balanced accuracy.
2.  Discuss in general how to choose the $\beta$ value for $F_\beta$-score.
:::

## Exercises

1.  Define the following metrics for performance evaluation of classifiers: Accuracy, Precision, Recall, Sensitivity, Specificity, $F_1-score$, $F_\beta-score$

2.  Given a confusion matrix having the truth by columns and the predictions by rows. The matrix is used to evaluate a 9-class classifier.

```{=html}
<!-- -->
```

        [[45  0  0  0  0  0  0  0  0  0]
         [ 0 45  0  0  0  0  0  0  0  0]
         [ 0  0 54  0  0  0  0  0  0  0]
         [ 0  0  0 42  0  1  0  1  0  0]
         [ 0  0  0  0 49  0  0  1  0  0]
         [ 0  0  0  0  0 38  0  0  0  0]
         [ 0  0  0  0  0  0 42  0  0  0]
         [ 0  0  0  0  0  0  0 45  0  0]
         [ 0  1  1  2  0  0  0  0 39  1]
         [ 0  0  0  0  1  0  0  0  1 41]]

Explain row 9 of the confusion matrix. And what is the precision and recall of the classifier for the 9th class?

3.  Assume a binary classifier using `yes` as positive class and `no` as negative class. Given the following truth and prediction data.

```{=html}
<!-- -->
```
        truth: "yes" "yes" "yes" "no" "no" "no" "no" "no" "no" "yes" 
        prediction: "no" "yes" "no" "yes" "yes" "no" "no" "yes" "yes" "no"

-   Calculate precision, recall/sensitivity, specificity, balanced accuracy.
-   Discuss in general how to choose the $\beta$ value for $F_\beta$-score.




## References





