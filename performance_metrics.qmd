---
title: Performance Metrics
---

::: callout-important
#### text section & script

[@deitel_intro_2019] Section 15.3.1 Metrics for Model Accuracy

Study both [the text's Jupyter Notebook](https://github.com/pdeitel/IntroToPython/blob/master/examples/ch15/snippets_ipynb/15_02-03.ipynb) and the code on this page.
:::

## Metrics for Model Scoring {#performancemetric}

We use the following confusion matrix to learn a few performance metrics.

|              |   prediction   |               |
|-------------:|:--------------:|:-------------:|
|        truth | *0 / negative* | *1 /positive* |
| 0 / negative |     TN = 1     |    FP = 2     |
| 1 / positive |     FN = 3     |    TP = 4     |

: An example of confusion matrix

::: callout-important
### QUESTION: Defining evaluation metrics for classifier evaluation

Define the following metrics for performance evaluation of classifiers: Accuracy, Precision, Recall, Sensitivity, Specificity, $F_1-score$, $F_\beta-score$
:::

### Accuracy

We define **overall success rate** (or accuracy) as a metric defining what the classifier has correctly done.

-   **The ratio** between the sum of the diagonal values (i.e., TP and TN) vs. the sum of the matrix.

$$
accuracy = \frac{TP+TN}{TP+TN+FP+FN} = \frac{4+1}{10}= 0.5
$$

In other words, the confusion matrix of a good model has large numbers diagonally and small (ideally zero) numbers off-diagonally.

### Precision, Recall, Sensitivity, Specificity

`Precision` and `Recall` are the two *accuracy metrics* used in the **information retrieval** community; they are often used to characterize classifiers as well.

::: panel-tabset
#### Precision

**Precision** is the percent of cases we (the model) marked `positive` really are positive.

This tells us how **reliable** it is when a classifier has marked a case as positive.

$$
precision = \frac{TP}{TP+FP} = \frac{4}{4+2} \approx 0.667
$$ In a classification report, the precision specifies the total number of correct predictions for a class divided by the total number of that class in the predictions.

#### Recall & Sensitivity

**Recall** is the percent of `positive` cases we were able to correctly identify and select. Recall is also called **Sensitivity** and **True Positive Rate**, the portion of *true positive cases* that a classifier is able to discover.

$$
recall = TPR = \frac{TP}{TP+FN} = \frac{4}{4+3}\approx 0.571
$$ In a classification report, the recall specifies the total number of correct predictions for a class divided by *the total number of that class in the truth*.

#### Specificity

**Specificity**, also called **True Negative Rate**, is a metric to evaluate how good a classifier `identifies negative class`, calculated as the portion of the negative cases which has identified by a classifier.

$$
specificity = TNR = \frac{TN}{TN+FP} = \frac{1}{1+2} \approx 0.333
$$

The **false positive rate**, FPR, is $$
FPR = \frac{FP}{TN+FP} = 1-specificity
$$
:::

::: callout-important
## QUESTION: Reading Confusion Matrix

Given a confusion matrix having the truth by columns and the predictions by rows. The matrix is used to evaluate a 10-class classifier.

    [[45  0  0  0  0  0  0  0  0  0]
     [ 0 45  0  0  0  0  0  0  0  0]
     [ 0  0 54  0  0  0  0  0  0  0]
     [ 0  0  0 42  0  1  0  1  0  0]
     [ 0  0  0  0 49  0  0  1  0  0]
     [ 0  0  0  0  0 38  0  0  0  0]
     [ 0  0  0  0  0  0 42  0  0  0]
     [ 0  0  0  0  0  0  0 45  0  0]
     [ 0  1  1  2  0  0  0  0 39  1]
     [ 0  0  0  0  1  0  0  0  1 41]]

Explain row 9 of the confusion matrix. And what is the precision and recall of the classifier for the 9th class?
:::

### Balanced Accuracy

Accuracy is used to evaluate how good a binary classifier is. When the two classes are **imbalanced**, i.e., one class appears more frequently than the other, the balanced accuracy should be taken as the average of the `sensitivity score for positive` class and the `specificity score for negative class`. $$
\text{balanced accuracy} = \frac{sensitivity+specificity}{2} \approx 0.452
$$





### $F_1$-score

In a binary classification, `F-score` or `F-measure` is calculated from the **precision** and **recall** of the test. The traditional F-score, $F_1-score$, is the average of precision and recall.

The highest possible value of an $F_1$-score is 1 and the lowest is 0.

$$
F_1\text{} = \frac{precision+recall}{2} \approx 0.615
$$


::: column-margin

In his 1979 book Information Retrieval, Cornelis Joost van Rijsbergen defined a function very similar to the F-score, recognizing the inadequacy of accuracy as a metric for information retrieval systems. He called the metric the Effectiveness function, and assigned it the letter E, because it measures the _effectiveness of retrieval with respect to a user who attaches_ $\beta$ _times as much importance to recall as precision_. It is not known why the F-score is assigned the letter F today.
:::


### $F_\beta$-score

A more general F-score, $F_\beta$, uses a positive factor $\beta$ by which the score will weigh recall $\beta$ times as important as precision.

Recall measures how sensitive a model is to the positive class. Precision is about how many cases predicted as positive are really positive.

::: callout-tip
#### Choosing the $\beta$ value for $F_\beta$-score

If discovering all the positive cases is more important than filtering false positive cases, then $\beta$ should be greater than $1$.
:::

Two commonly used values for $\beta$ are $2$, which weighs recall higher than precision, and $0.5$, which weighs recall lower than precision.

$$
F_\beta\text{} = (1+\beta^2)\times\frac{precision\times recall}{\beta^2\times precision + recall} 
$$

::: column-margin
A loss or cost function is a technique which is used to represent the loss or cost of an event, such as mistakenly classifying a negative case as positive or vice verse.
:::

::: callout-important
## QUESTION: Evaluating a two-class classifier

Assume a binary classifier using `yes` as positive class and `no` as negative class. Given the following truth and prediction data.

    truth: "yes" "yes" "yes" "no"  "no"  "no"  "no"  "no"  "no"  "yes"
    prediction: "no"  "yes" "no"  "yes" "yes" "no"  "no"  "yes" "yes" "no"

1.  Calculate precision, recall/sensitivity, specificity, balanced accuracy.
2.  Discuss in general how to choose the $\beta$ value for $F_\beta$-score.
:::

## Exercises

### 1

Define the following metrics for performance evaluation of classifiers: Accuracy, Precision, Recall, Sensitivity, Specificity, $F_1-score$, $F_\beta-score$

### 2

Given a confusion matrix having the truth by columns and the predictions by rows. The matrix is used to evaluate a 9-class classifier.

```{=html}
<!-- -->
```

        [[45  0  0  0  0  0  0  0  0  0]
         [ 0 45  0  0  0  0  0  0  0  0]
         [ 0  0 54  0  0  0  0  0  0  0]
         [ 0  0  0 42  0  1  0  1  0  0]
         [ 0  0  0  0 49  0  0  1  0  0]
         [ 0  0  0  0  0 38  0  0  0  0]
         [ 0  0  0  0  0  0 42  0  0  0]
         [ 0  0  0  0  0  0  0 45  0  0]
         [ 0  1  1  2  0  0  0  0 39  1]
         [ 0  0  0  0  1  0  0  0  1 41]]

Explain row 9 of the confusion matrix. And what is the precision and recall of the classifier for the 9th class?

### 3  

Assume a binary classifier using `yes` as positive class and `no` as negative class. Given the following truth and prediction data.

```{=html}
<!-- -->
```
        truth: "yes" "yes" "yes" "no" "no" "no" "no" "no" "no" "yes" 
        prediction: "no" "yes" "no" "yes" "yes" "no" "no" "yes" "yes" "no"

-   Calculate precision, recall/sensitivity, specificity, balanced accuracy.
-   Discuss in general how to choose the $\beta$ value for $F_\beta$-score.



### 4

A binary classifier needs to be employed in a medical screening and diagnosis application. You need to evaluate the classifier according to the given confusion matrix and its associated classification report. 

Assume that 0 and 1 denote positive and negative classes, respectively. The class distribution in the training data is unevenly distributed. About 20% of cases are positive and the remaining ones are negative. 

```{python}
#| echo: false
#| results: hold
#| eval: true
#| warning: false
#| message: false
#| 
import dask
import numpy as np
import dask.array as da
import dask.dataframe as dd
# from dask import delayed

dummy_y = dd.read_csv(
  "data/dummy_y_file.csv", 
  blocksize=25e6, 
  header=None, 
  names=['y_true','y_pred'], 
  dtype={'y_true':np.int32,'y_pred':np.int32})

labels = sorted(dummy_y['y_true'].unique().compute().tolist())
print(f'Labels: {labels}')
# inspect columns
print(f"Class distribution (percentage) in the training set: \n{(dummy_y['y_true'].value_counts().compute())/(len(dummy_y))}")
```

The given confusion matrix is:

```{python}
#| echo: false
#| results: hold
#| warning: false
#| message: false

# confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator

cmat = confusion_matrix(y_true=dummy_y['y_true'], y_pred=dummy_y['y_pred'], labels=labels)
#print(cmat)
```

```{r echo=F,eval=T}

df <- as.data.frame(py$cmat)
names(df) <- c("y_pred = 0", "y_pred = 1")
knitr::kable(as.data.frame(df, row.names = c("y_true = 0", "y_true = 1")))

```

```{python}
#| include: false
#| warning: false
#| message: false
# visualize matrix
# The viridis scale is good for colour blindness
fig, ax = plt.subplots()
img = ax.imshow(cmat, interpolation='nearest', cmap=plt.cm.get_cmap("viridis"))
_ = ax.set_title('Confusion matrix')

_ = fig.colorbar(img)
ticks = np.arange(len(labels))
_ = ax.set_xticks(ticks)
_ = ax.set_yticks(ticks)
_ = ax.set_ylabel('Truth')
_ = ax.set_xlabel('Prediction')
_ = ax.minorticks_on()
_ = ax.xaxis.set_minor_locator(AutoMinorLocator(2))
_ = ax.yaxis.set_minor_locator(AutoMinorLocator(2))
_ = ax.grid(linestyle = '--', which='minor')
_ = plt.show()
```


And its associated classification report is:

```{python}
#| echo: false
#| results: hold
#| warning: false
#| message: false

report = classification_report(
  y_true=dummy_y['y_true'], 
  y_pred=dummy_y['y_pred'], 
  labels=labels, # set 0 as positive, 1 negative
  target_names = ['positive(0)', 'negative(1)'])
print(f'''\n{report}''')
```

Assume that the classifier is required to have a high rate for both reliability and discovery of the positive class. Per the standard performance metrics, precision can measure the reliability of positive class, and recall can measure the discovery rate of positive class.


Answer the questions:

1. Which cell(s) of confusion matrix should ideally be zero or minimized?


2. As both precision and recall should be included in the performance evaluation, take $F_\beta-score$ as the performance metric.

$$
F_\beta\text{} = (1+\beta^2)\times\frac{precision\times recall}{\beta^2\times precision + recall} 
$$

Determine the value of $\beta$ and calculate $F_\beta-score$ for each of the following cases:


- Both precision and recall for positive class should equally be weighed.

- Discovering all the positive patients are more important than how reliable when the classifier has marked a case as being positive

- The reliability of marking positive cases accounts more than recalling all the true positive cases.















<!--

You may use the link as reference: https://cisjw.sitehost.iu.edu/ai/key.html#questions



The paper "Implications of False-Positives for Future Cancer Screenings"(https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5992010/) studied 10 years of electronic medical records data and analyzed the association between prior receipt of a false-positive cancer screening test result and future participation in routine cancer screenings.

The following conclusion was drawn from the study:

"Prior experience with cancer screenings may influence a patient’s willingness to continue future screening. Patients who previously experienced a false-positive breast or prostate cancer screening were more likely to engage in future screening for all cancers, while women with a false-positive colorectal cancer FOBT screening were less likely to engage in future breast cancer screening. "

Regarding the FP rate, the data from the paper showed that FP is common. "50–61% of women undergoing annual mammography and 10–12% of men undergoing regular PSA testing will experience a false-positive result."1–4

-->

## References





