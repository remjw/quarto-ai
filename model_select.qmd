---
title: The _"Best"_ Model
format: 
  html: 
    code-fold: true 
    code-tools: true
    code-link: true
#execute:
#  echo: false 
#  cache: true
---


::: callout-tip
### No Free Lunch Theorem

> Yet-Another Philosophical Question: Should we search for the BEST model?

**"A model is a simplified version of the given observations. The simplifications are intended to discard the superfluous details that are unlikely to be generalized to new instances. To decide what data to discard and what data to keep, you must make assumptions."**[@aurelien_geron_hands-machine_2022]

[In a famous 1996 paper](https://homl.info/8), David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other, referred to as **No Free Lunch (NFL)** theorem. 

There is no model that is a priori guaranteed to work better. The only way to know the best model is to evaluate them ALL. Apparently this is impossible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. 

:::


Assume the best model exists under certain assumptions. Most of us have already been talking generally about machine learning and AI applications as they have invaded every aspect of our lives in good way. To take serious advantage of the relevant technologies, there are a lot of questions to be taken into consideration beyond algorithms, complexity and accuracy.

> To Adopt or Abandon?: Give concrete and convincing reasons and examples to persuade a business or an organization or an institute to either adopt or abandon machine learning technology. 

> Technical Question: What techniques are currently available to distinguish the good models from the bad ones and determine the best one from a short-list?

> Fixed or Time-Variant Model? : Should a model never be changed once it has been deployed and being used in a production environment? How often should an existing machine learning model be regularly re-evaluated? What would you do if an obvious performance degrading has recently been detected from a running model?

> Offline vs. online/real-time model re-training. Training cost. Data quality & credibility. Data privacy.

::: {.callout-tip .column}
#### The lesson is an extended discussion of

-   [@deitel_intro_2019] Sections 15.3.3, 15.3.4
:::

A majority of machine learning problems can be solved in various ways. The family of *supervised machine learning* algorithms is purposed to search for a discriminative model to maximize disparities between classes/categories. This type of algorithm requires a specific collection of *labeled/categorized/example data* prior to its launch, and fits a classifier model to the given data. The data dependency results in over-fitting as well as possible performance degrading arising from data drift or concept drift.

::: {.callout-note .column-margin}
## Concept Drift & Data Drift

*Wikipedia definition*: In predictive analytics and machine learning, *concept drift* means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.
:::

The algorithm outputs a classifier which is an executable object. The classifier is intended to assign/predict a label to each new, formerly unseen object with satisfactory performance. To name a few,

-   Logistic Regression
-   *Stochastic Gradient Descent (SGD)*
-   K-nearest Neighbors
-   Na√Øve Bayes [@noauthor_naive_nodate]
-   *Support Vector Machine (SVC)*
-   Decision Tree
-   Random Forest

Training a machine learning model may require one or more parameters be presumed beforehand, referred to as `hyperparameters`. As hyperparameters can have a direct impact on the model training process. It is important to understand how to optimize them. This has led to research and development of various strategies and algorithms. 

::: {.callout-note .column-margin}
## Hyperparameters & Model Parameters

In machine learning, a **hyperparameter** is a parameter whose value must be manually specified before model fitting begins. Hyperparameters are used to control the learning process and they directly impact the training process. 

By contrast, **model parameters** are the values of other parameters (typically node weights) which are derived via training and directly related to training data. 

:::

For instance, the following lists the hyperparameters in some popular algorithms.

::: panel-tabset
### K-nearest Neighbors

Find an optimal number of neighbors $K$ which maximizes model performance.

### Linear Model

Hyperparameter: Find an optimal integer for number of independent features $n$ in a linear model.

- $b, a_1-a_n$ are model parameters which are calculated by an algorithm.

$$
y = b + a_1x_1 + \dots + a_nx_n
$$

### Decision Tree

-   The maximum depth, i.e., height of tree
-   The minimum number of samples required in a leaf node

```{r}
#| echo: false
#| column: screen-inset-shaded
#| layout-nrow: 1
library(ISLR)
library(rpart)
library(rpart.plot)
#build the initial decision tree
tree <- rpart(Salary ~ Years + HmRun, data=Hitters, control=rpart.control(cp=.0001))

rpart.plot(tree)

#identify best cp value to use
best <- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]

#produce a pruned tree based on the best cp value
pruned_tree <- prune(tree, cp=best)
#plot the pruned tree
prp(pruned_tree)
```

### Random Forest

-   The number of trees in a random forest

### Gradient Descent

-   The learning rate $\alpha \in [0.0, 1.0]$ used as a multiplier in the term $\alpha \times \nabla$. It controls the speed of learning and dynamically determines the step size for next iteration.
-   The stopping condition to terminate the loop.

### Neural Network

-   The number of layers
-   The number of neurons per layer
:::



In a machine learning project, after completing data collection and preparation, there are at least two problems to be solved.

::: {.callout-important appearance="simple"}
### Two Problems

1.  Firstly, for a given dataset, which algorithm(s) would perform best for a specific use case in terms of chosen metrics?

2.  Secondly, each algorithm has one or more peculiar hyperparameters. After determining a short-list of the algorithm candidates, how do we determine the best value of each hyperparameter?
:::



This lesson discusses the techniques relevant to model section and tuning in a single-model scenario. A model selection technique trains multiple models and compares their performance for a given dataset. A model optimization technique determines the best configuration for a specific model.

Besides replying on a single algorithm, it is common practice to run two or more algorithms and consolidate the results in the final output, referred to as **ensemble modeling**. Random forest ensembles individual trees in a single model.

## ExampleSet

We randomly generate a numerical dataset of $N$ samples and train a binary classifier from the samples. The example data contains two independent features $x1$ and $x2$, and one class label $y$. The code cells in the lesson use the example set unless a different dataset is specified.

::: callout-tip
A one-third hold-out ratio is used to separate data into the training and test sets. The subsequent process for model selection is performed on the training set, `X_train` and `y_train` only.
:::

```{python}
#| label: Generate a Random ExampleSet
#| results: hold
#| eval: true
#| layout-ncol: 2

# 1. Data Generation 
import numpy as np
# configuration
np.random.seed(99) 
labels = [0, 1]
multiplier = 20
N = 5 * multiplier
N_folds = 5 # CV
holdout = 1/3 # ratio 
# A dataset of 2 features x1, x2 & 1 label y
y = np.random.choice(labels, size=N, replace=True)
x1 = np.array([0.1, 0.2, 0.3, 0.4, 0.5] * multiplier)
x2 = np.random.sample(N)
X = np.stack((x1, x2), axis=-1)

# 2. Holdout 70% for training, 30% for test
from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=holdout, shuffle=True, random_state=0)

# inspect head
print(f'''
Head of X:
{X[0:5]}
Head of y:
{y[0:5]}
''')
# class distribution
print(f'''
CLASS DISTRIBUTION after Stratified Split
y:
Percentage of label 0: {round(sum(y==0)/len(y),5)}
Percentage of label 1: {round(sum(y==1)/len(y),5)}
y_train:
Percentage of label 0: {round(sum(y_train==0)/len(y_train),5)}
Percentage of label 1: {round(sum(y_train==1)/len(y_train),5)}  
y_test:
Percentage of label 0: {round(sum(y_test==0)/len(y_test),5)}
Percentage of label 1: {round(sum(y_test==1)/len(y_test),5)}  
''')
```



## Model Selection

Without model comparison, it is hard to anticipate the best performed model type for a given dataset. Empirical experience can firstly be used to initially filter the candidates. The selection process can subsequently be executed to compare the candidates.

### Determine Candidates

::: panel-tabset
#### Use the Estimator Diagram

The following is a useful diagram for choosing the right type of estimator based on the three conditions: `data type`, `data size` and `learning type`.

```{r echo=F}
knitr::include_url("https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html")
```

#### To Determine Candidates

1.  KNeighbors Classifier
2.  Logistic Regression
3.  SGD Classifier (*Stochastic Gradient Descent*)
4.  SVC (*Support vector machine*)
:::

### Compare Candidates

::: panel-tabset
#### Problem

Use Scikit-Learn to compare the classification algorithms determined from the last step (*All models are trained with default parameters*.):

1.  `KNeighborsClassifier`
2.  `LogisticRegressioin`
3.  `SGDClassifier`
4.  `SVC` (Support vector machine)

#### How: Scikit-Learn APIs

```{python}
#| results: hold
#| eval: true

# continue
# 3. Determine Model Candidates
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# 4. group estimator names and objects in a dictionary
estimators = {
  'KNeighborsClassifier': KNeighborsClassifier(algorithm='auto',n_neighbors=5), 
  'LogisticRegression': LogisticRegression(random_state=0),
  'SGDClassifier': SGDClassifier(alpha=0.00001, max_iter=1000, early_stopping=False, tol=0.001),
  'SVC': SVC(gamma='scale')
  }

for estimator_name, estimator_object in estimators.items():
  # kfold = KFold(n_splits=2, random_state=11, shuffle=True)
  scores = cross_validate(estimator_object, X_train, y_train, cv=N_folds, return_train_score=True, return_estimator=True)
  print(f'{estimator_name:>20}: ' + 
           f'mean accuracy={scores["test_score"].mean():.2%}; ' +
           f'standard deviation={scores["test_score"].std():.2%}')
```
:::

### Exercise

::: callout-tip
#### Question: Determining Model Candidates

How would you modify the code given above so that it would also test a Naive Bayes classifier model, `'GaussianNB': GaussianNB()`?
:::

## Model Tuning

Once we have identified a shortlist of models in terms of specific metrics, we should fine-run them to find hyperparameter values that produce the best possible results.

::: callout-tip
Model tuning runs K-fold cross validation over the training data.
:::

### Grid Search

The Grid search technique is a brute-force method which explores every combination globally in one-/multi-dimensional parameter space, do model evaluation and search for the best parameter values.

::: callout-note
### Grid Search from Scratch

Not required `numpy.arange` `numpy.meshgrid`
:::

### 1D Grid Search

::: panel-tabset

#### Search Optimal K for K-NN Classifier

1.  Get the samples

2.  Split samples to training and test sets

3.  Get the training set

4.  Create a K-NN classifier object

5.  Set $K_{min}$, $K_{max}$, and $stepsize$ to define search grid and its granularity

6.  Loop: For each integer $k \in [K_{min}, K_{max}, stepsize]$

    1.  Do K-fold cross validation with training set

    2.  Record $k$ and its score

7.  Find the k value having the best score

8.  Get the best or top-n estimators

#### 1.Create Model & Grid_Search Objects

Scikit-Learn `model_selection.GridSearch` API

```{python}
#| results: hold
#| output-location: fragment
# Grid search w/ CV
import numpy as np
from time import time
import scipy.stats as stats

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV

# Create KNN classifier object 
knn = KNeighborsClassifier()

# Define grid
param_grid = {
    "n_neighbors": np.arange(1,21)
}
# number of folds
N_folds = 5

# Create grid search object
grid_search = GridSearchCV(knn, param_grid=param_grid, cv=StratifiedKFold(n_splits=N_folds, shuffle=False))

# Run grid search
start = time()
grid_search.fit(X_train, y_train)

```


#### 2. View Output Structure `cv_results_`

```{python}
for obj in grid_search.cv_results_:
  print(obj)

```


#### 3. Reporting 

```{python}
# Utility function to customize the report 
def report(results, n_top=3):
  for i in range(1, n_top + 1):
    candidates = np.flatnonzero(results["rank_test_score"] == i)
    for candidate in candidates:
      print(f'''
      Model with rank = {i}
      Mean validation score: {results["mean_test_score"][candidate]:.3f} 
      std: {results["std_test_score"][candidate]:.3f}
      Parameters: {results["params"][candidate]}
      ''')

# 
print(f'''
GridSearch with {N_folds}-Fold CV took {time()-start:.2f} seconds for {len(grid_search.cv_results_["params"])} candidate parameter settings.

Best_estimator: {grid_search.best_estimator_}
''')

# display top-k (3 default) estimators for parameters and score
report(grid_search.cv_results_)
```

#### 4. Plotting `mean_test_score` vs. $K$

```{python}
#| results: hold
import matplotlib.pyplot as plt
import numpy as np
np.random.seed(99)

ks = grid_search.cv_results_["param_n_neighbors"]
scores = grid_search.cv_results_["mean_test_score"]
print(ks, scores)
colors = np.random.rand(len(ks))
area = scores * 20
plt.scatter(ks, scores, s=area, c=colors, alpha=0.5)
#plt.plot(np.append(scores[0],scores), linestyle="dotted")
plt.title('Change of mean_test_score over $K$ values')
plt.ylabel('mean_test_score')
plt.grid()
plt.show()
```


#### 5. Conclusion


:::


### 2D-Grid Search

Tuning two parameters simultaneously requires a 2-dimensional grid.


#### Create 2D-Meshgrid

Each grid/dot represents one value pair of the combination (x, y)

```{python}
#| eval: true
#| echo: true
#| layout-ncol: 2
#| 
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
import numpy as np
 
# Define grid 
x = np.linspace(0,3,4)
y = np.linspace(0,3,4)
X, Y = np.meshgrid(x,y)

print(f'''
x = {x}
y = {y}
X = 
{X}
Y = 
{Y}
''')
 
#for pair in zip(X[0,:], Y[0,:]):
#    print(pair)
    
Z= X**2 + Y**2

print(f'''Z = $X^2 + Y^2$
{Z}''')

# 2d plot
fig = plt.figure()
_ = plt.contourf(X,Y,Z,cmap='plasma')
_ = plt.axis('scaled')
_ = plt.colorbar()
#plt.savefig('contour1.png')
plt.show()

# 3d plot
fig = plt.figure()
ax = plt.axes(projection='3d')
_ = ax.plot_surface(X, Y, Z, cmap="plasma", linewidth=0, antialiased=False, alpha=0.5)
_ = ax.set_xlabel('x')
_ = ax.set_ylabel('y')
_ = ax.set_zlabel('z');
# rotation
_ = ax.view_init(30, 60)
#plt.savefig('3Dplot1.png',dpi=600)
plt.show()
```





#### Exercise

::: callout-tip
#### Question: Meshgrid Plotting in 2D & 3D Plot 

Write code to replicate the same 2D and 3D plots which visualize the function $Z = X^2 + Y^2$s. Set $x\in[-6, 6], y\in[-6,6], z = x^2 + y^2$.

```{python}
#| eval: true
#| echo: false
#| warning: false
#| message: false
#| layout-ncol: 2
#| 
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
import numpy as np
 
# Define grid 
x = np.linspace(-6,6,13)
y = np.linspace(-6,6,13)
X, Y = np.meshgrid(x,y)
 
#for pair in zip(X[0,:], Y[0,:]):
#    print(pair)
    
Z= X**2 + Y**2

# 2d plot
fig = plt.figure()
_ = plt.contourf(X,Y,Z,cmap='plasma')
_ = plt.axis('scaled')
_ = plt.colorbar()
#plt.savefig('contour1.png')
plt.show()

# 3d plot
fig = plt.figure()
ax = plt.axes(projection='3d')
_ = ax.plot_surface(X, Y, Z, cmap="plasma", linewidth=0, antialiased=False, alpha=0.5)
_ = ax.set_xlabel('x')
_ = ax.set_ylabel('y')
_ = ax.set_zlabel('z');
# rotation
_ = ax.view_init(30, 60)
#plt.savefig('3Dplot1.png',dpi=600)
plt.show()
```

:::



### Tune SGD Classifier

::: panel-tabset


#### Define 2D Grid ($\alpha, tol$)

To fine-tune a SGD classifier, define a 2-dimensional grid for the learning rate $alpha$ and stopping condition $tol$.

```{python}
#| eval: true
#| echo: true
n_alpha, n_tol = 20, 10
param_grid = {
    "alpha": np.linspace(0.6, 1, n_alpha),
    "tol": np.linspace(0.001, 0.01, n_tol)
}
print(f'''
alpha: {param_grid["alpha"]}
tol: {param_grid["tol"]}
''')
```

#### Create Objects

```{python}
#| results: hold
#| 
# Grid search w/ CV
import numpy as np
from time import time
import scipy.stats as stats

from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV

# default: alpha=0.00001, max_iter=1000, early_stopping=False, tol=0.001
sgd = SGDClassifier()
# Define grid
n_alpha, n_tol = 20, 10
param_grid = {
    "alpha": np.linspace(0.6, 1, n_alpha),
    "tol": np.linspace(0.01, 0.1, n_tol)
}
N_folds = 5
# run grid search
grid_search = GridSearchCV(sgd, param_grid=param_grid, cv=StratifiedKFold(n_splits=N_folds, shuffle=False))

start = time()
grid_search.fit(X_train, y_train)

```

#### View `cv_results_`

```{python}
for obj in grid_search.cv_results_:
  print(obj)

```

#### Reporting

```{python}
#| results: hold

# Utility function to customize report
def report(results, n_top=3):
  for i in range(1, n_top + 1):
    candidates = np.flatnonzero(results["rank_test_score"] == i)
    for candidate in candidates:
      print(f'''
      Model with rank = {i}
      Mean validation score: {results["mean_test_score"][candidate]:.3f} 
      std: {results["std_test_score"][candidate]:.3f}
      Parameters: {results["params"][candidate]}
      ''')
      
print(f'''
GridSearch with {N_folds}-Fold CV took {time()-start:.2f} seconds for {len(grid_search.cv_results_["params"])} candidate parameter settings.
Best_estimator: {grid_search.best_estimator_}
''')

report(grid_search.cv_results_, n_top=1)
```



#### Plotting `(alpha,tol)` vs. `mean_test_score`

```{python}
#| eval: true
#| results: hold

# Display 2d data in a 2d plot
# Display 2d data in a 2d plot
import matplotlib.pyplot as plt
import numpy as np
np.random.seed(99)

alpha = np.linspace(0.6, 1, n_alpha)
tol = np.linspace(0.1, 0.01, n_tol)
scores = grid_search.cv_results_["mean_test_score"]
print(scores[0:5])

A, T = np.meshgrid(alpha, tol)
print(A.shape, T.shape)
# 
Z = np.flipud(np.reshape(scores, (n_tol, n_alpha)))
_ = plt.clf()
_ = plt.contourf(A, T, Z, 20, cmap='RdGy') #RdGy
_ = plt.colorbar()
_ = plt.xlabel("$alpha$")
_ = plt.ylabel("$tol$")
_ = plt.title('How does mean_test_score vary with $(alpha, tol)$?')
_ = plt.show()

```
:::



### Exercise

::: callout-tip
### Question: Hyperparameter Tuning

How would you modify the code given above so that it would fine-tune the following two hyperparameters for SGD classifier?

-   $alpha \in [0.5, 1]$
-   $max\_iter \in [50, 100]$
:::

## Case Study

Sections 15.4 and 15.5


## References

::: {#refs}
:::
