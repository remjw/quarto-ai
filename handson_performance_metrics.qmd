---
title: Hands-on Performing Model Evaluation with scikit-learn 
---



The hands-on performs model evaluation by using the **metrics module** from the **scikit-learn** library, a popular machine learning library. 


## The Format of Confusion Matrix

The format of the `confusion_matrix` function in the *sklearn.metrics module* is shown as below:

|              |          |                      |                      |
|--------------|----------|----------------------|----------------------|
|              |          | ***y_pred***         |                      |
|              |          | *positive*           | *negative*           |
| **y\_ true** | positive | ( true *positive )*  | ( false *negative )* |
|              | negative | ( false *positive )* | ( true negative )  |

: The format of confusion matrix from the sklearn.metrics module

Each row corresponds to one truth class. $y\_true$ is the name of the variable which stores the truth. $y\_pred$ is the name of the variable for the predictions. The matrix assumes the first label as the positive class, if not specified.

::: column-margin
::: callout-note
#### Check scikit-learn
To check if scikit-learn is available, run the import statement:
```{python}
from sklearn.metrics import confusion_matrix
```
If not, install it by running in the terminal: *pip3 install scikit-learn*
:::
:::

## Official Example

In the following embedded frame (or open the page in new tab), read the official document of the `confusion_matrix` function.

```{r echo=FALSE}
knitr::include_url("https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html")
```


## Toy Example

A simple example is given below with three parts separated into **three tabs** in their execution order.

::: panel-tabset

### Create confusion_matrix


```{python echo=TRUE,eval=TRUE}
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
# define sample labels, truth and prediction
truth =      [1,0,1,1,1,0,0,1,1,1]
prediction = [1,1,1,0,0,1,0,1,1,0]
# confusion matrix
# set labels argument to define first label 1 as positive class
cmat = confusion_matrix(y_true=truth, y_pred=prediction, labels=[1,0]) 
print(cmat)
```


::: callout-important
#### Determine the arguments in the sklearn.metrics.confusion_matrix method

- The **labels parameter** by default assumes 0 as positive class and 1 as negative.
- To specify a different setting, the argument `labels=[1,0]` in line 8 defines that there are two classes 1 and 0, and set the first label 1 as the positive class.
:::

::: callout-tip
#### Passing string labels to sklearn.metrics.confusion_matrix

-   To pass string labels e.g., yes and no, and yes is positive class, pass `['yes', 'no']` to the **labels** parameter. 
-   The labels option may also be used to reorder or select a subset of class labels.
-   If `None` is passed to **labels**, those that appear at least once in `y_true` or `y_pred` are used as class labels in ascending order.

:::



### Read TP, FP, TN, FN

For a binary classifier, extract the four counts as follows:

```{python echo=TRUE,eval=TRUE}
tp,fn,fp,tn = cmat.ravel()
print(tp,fn,fp,tn)
```

### Scoring

precision_score, recall_score, f1_score

```{python echo=TRUE,eval=TRUE}
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
# Define sample labels, truth and prediction
truth = [1,0,1,1,1,0,0,1,1,1]
prediction = [1,1,1,0,0,1,0,1,1,0]
# scores
print(f"precision={precision_score(y_true=truth, y_pred=prediction)}")
print(f"recall={recall_score(truth, prediction)}")
print(f"f1-score={f1_score(y_true=truth, y_pred=prediction)}")
```

:::



## Handling String Labels

The following gives an example of creating confusion matrix for a binary classifier using two string labels *yes* and *no*. The positive class is set to *yes*.

```{python}
import numpy as np
from sklearn.metrics import confusion_matrix
truth = ["yes","yes", "yes" ,"no" , "no" , "no" , "no" , "no" , "no" , "yes"]
prediction = ["no",  "yes", "no" , "yes", "yes", "no" , "no" , "yes", "yes" ,"no"]
# Create confusion matrix
confusion_mat = confusion_matrix(
  y_true=truth, 
  y_pred=prediction, 
  labels=["yes", "no"])
print(confusion_mat)
```


## Classification Report

When a summary report is required, **sklearn.metrics.classification_report** generates a classification performance report. [Link to the official examples of using the report function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).



However, the metrics in the classification report from `sklearn` are not exactly the same as the standard definitions of the metrics. 

We first run the cell below to display a confusion matrix and use it as the example to explain three metrics: `support`, `macro avg` and `weighted avg`.

```{python echo=TRUE,eval=TRUE}
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
truth = [1,0,1,1,1,0,0,1,1,1]
prediction = [1,1,1,0,0,1,0,1,1,0]
report = classification_report(
    y_true=truth, 
    y_pred=prediction, 
    labels=[1,0])
print(report)
```

::: callout-important
#### The three metrics in the sklean.metrics.classification_report

1.   **support**: The number of each class label in the truth. As we can see from the `truth` variable in line 4 of the script cell, the test data has *7 positive (1) cases* and *3 negative (0) cases*.

2.   **macro avg**: Calculate the metrics for each label, and find their un-weighted mean. This does not take label imbalance into account. For example, the precision of positive class is 0.67 and negative class is 0.25; **macro avg** of precision is $(0.67+0.25)/2=0.46$.

3.   **weighted avg**: Calculate metrics for each label, and find their **average weighted by support**. For example, the **weighted avg of the precision** is calculated as

$$
0.67\times \frac{7}{10} + 0.25 \times \frac{3}{10} = 0.544
$$

The `weighted-avg metric` alters `macro-avg` to **account for label imbalance**.

:::

## Multi-Class Classifier

To perform the same performance evaluation for more than two classes, the following script, split into two tabs, deals with a classification problem of five classes labeled as `0,1,2,3,4`. The first tab creates confusion matrix and the second table prints the classification report.

::: panel-tabset

### Confusion Matrix

```{python echo=T, eval=T, results='hold'}
#| cap-location: margin

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# make two lists for expected/true and predicted/estimate target/label values
expected = [2, 0, 0, 2, 4, 4, 1, 0, 3, 3, 3]
predicted = [2, 1, 0, 2, 4, 3, 1, 0, 1, 3, 3]

# Create confusion matrix
cmat = confusion_matrix(expected, predicted)
print(cmat)
```


### Classification Report

The classification report lists several [classification metrics]() in a table. To get the report, run

```{python}
from sklearn.metrics import classification_report
# Classification report
class_names = ['Class-0', 'Class-1', 'Class-2', 'Class-3', 'Class-4']
report = classification_report(
  expected, 
  predicted, 
  target_names=class_names)
print(report)
```

:::


## Visualize Confusion Matrix

The following cell shows the code for creating a confusion matrix and visualizing the matrix in a color map.

```{python echo=T, eval=T, results='hold'}
#| cap-location: margin

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# make two lists for expected/true and predicted/estimate target/label values
expected = [2, 0, 0, 2, 4, 4, 1, 0, 3, 3, 3]
predicted = [2, 1, 0, 2, 4, 3, 1, 0, 1, 3, 3]

# Create confusion matrix
cmat = confusion_matrix(expected, predicted)

# Visualize confusion matrix
# The viridis scale is good for colour blindness
fig, ax = plt.subplots()
img = ax.imshow(cmat, interpolation='nearest', cmap=plt.cm.get_cmap("viridis"))
ax.set_title('Confusion matrix')

fig.colorbar(img)
ticks = np.arange(5)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_ylabel('Truth')
ax.set_xlabel('Prediction')
ax.minorticks_on()
ax.xaxis.set_minor_locator(AutoMinorLocator(2))
ax.yaxis.set_minor_locator(AutoMinorLocator(2))
ax.grid(linestyle = '--', which='minor')
plt.show()
```








## Exercises

### Concepts

Define precision, recall, f1-score, support

### Scripting

The following two cells generates two lists `y_true` and `y_pred` to simulate the truth and prediction, respectively.

```{python eval=TRUE, echo=TRUE}
# truth
import numpy as np
np.random.seed(99)
labels = [0, 1]  
y_true = np.random.choice(labels, size=100, replace=True)
print(f'truth: {y_true}')
```

```{python eval=TRUE, echo=TRUE}
# prediction
np.random.seed(1234)
y_pred = np.random.choice(labels, size=100, replace=True)
print(f'prediction: {y_pred}')
```

Do the following in a Python script or a Jupyter Notebook. To work with Jupyter Notebook, I highly recommend you to sign in to Google Colab with your Google account, and work with Jupyter notebooks on the Colab platform.

1. First copy the two code cells above to generate y_true and y_pred for the truth and prediction.

2. Create the confusion matrix and classification report. The following is the expected matrix and report.

```{python echo=FALSE}
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
# confusion matrix
cmat = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0,1])
print(cmat)
report = classification_report(
  y_true=y_true, 
  y_pred=y_pred, 
  labels=[0,1], 
  target_names = ['safe', 'not-safe'])
print(report)
```

3. Visualize the confusion matrix

```{python echo=FALSE}
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
# The viridis scale is good for colour blindness
fig, ax = plt.subplots()
img = ax.imshow(cmat, interpolation='nearest', cmap=plt.cm.get_cmap("viridis"))
ax.set_title('Confusion matrix')

fig.colorbar(img)
ticks = np.arange(2)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_ylabel('Truth')
ax.set_xlabel('Prediction')
ax.minorticks_on()
ax.xaxis.set_minor_locator(AutoMinorLocator(2))
ax.yaxis.set_minor_locator(AutoMinorLocator(2))
ax.grid(linestyle = '--', which='minor')
plt.show()
```



4. Answer the question: According to the classification report and the colormap, if the classifier is used for detecting videos that are safe for kids, `label 0 is safe` and `1 is not safe`, would you prefer the model with the performance report above? In the four color cells of confusion matrix, which cell should ideally have a zero count? Why?

<!-- a model which has high precision for label 0 and lower recall for 
label 0? -->



