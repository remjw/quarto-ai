---
title: "Confusion Matrix"
---

A confusion matrix is used to evaluate the goodness of a classifier model in performance evaluation. This name is used because the matrix makes it easy to visualize if a classifier model has confused or mislabeled two or more classes. The matrix records the number of occurrences of each possible combination of true and predicted labels.

::: column-margin
::: callout-important
#### text section & script

[@deitel_intro_2019] Section 15.3.1 Metrics for Model Accuracy

Study both [the text's Jupyter Notebook](https://github.com/pdeitel/IntroToPython/blob/master/examples/ch15/snippets_ipynb/15_02-03.ipynb) and the code on this page.
:::
:::

The same matrix is also referred to as contingency table or cross table in statistics.

We first built a confusion matrix by hand and discuss some relevant terms. Then we take a look at the *confusion_matrix and classification_report classes* from *Python's* **sklearn.metrics** module.

## Confusion Matrix by Hand

Assume a binary classifier has been trained to assign an integer label $1$ if the new picture contains one *cat* or more, and $0$ otherwise. If we are only interested in cat, the classifier is used as a cat detector. Thus, the label $1$ for the cat class is **positive class** and label 0 is **negative class**.

::: {.callout-note appearance="simple"}
#### Determine positive and negative class

Be aware that the class labels for positive and negative classes are user defined. For instance, if a classifier is purposed to identify cats, the positive class is cat and everything else should be in the negative class. The two labels can be the string values `cat` and `non-cat`.

Even though the label names can be arbitrary, the names are normally expected to indicate the meaning of two distinct classes.

Some of machine learning libraries requires integer labels. `cat` and `non-cat` must be converted to integer 1 and 0.

Most modern libraries offer the user option to specify the positive and negative class.
:::

### Truth

To evaluate the goodness of the classifier, we find ten pictures containing both cat and not-cat ones. Firstly, assign an id number to each picture and manually tag each with 1 for cat and 0 for everything else. These tags become the benchmark, termed as ground **truth** in the performance evaluation.

Assume the following `truth` with integer labels,

    id      : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
    truth   : 1, 0, 1, 1, 1, 0, 0, 1, 1, 1

which is equivalent to the truth with string labels:

    truth: 'cat', 'non-cat', 'cat', 'cat', 'cat', 'non-cat', 'non-cat', 'cat', 'cat', 'cat'

### Prediction

With the truth available, send the ten pictures to the classifier. The classifier will output/predict a class label, either 1 or 0, for each picture. Assume the following ten `predictions`:

    id          : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
    prediction  : 1, 1, 1, 0, 0, 1, 0, 1, 1, 0

::: column-margin
::: {.callout-note appearance="minimal"}
#### What is the prediction from a classifier?

A classifier recommends a most-likely class/group/category for a given data point/case , and outputs the class recommendation. The output class is the prediction which has made by the classifier.
:::
:::

### Structure

After collecting the prediction for each picture, place them with the truth together.

    id          : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
    prediction  : 1, 1, 1, 0, 0, 1, 0, 1, 1, 0
    truth       : 1, 0, 1, 1, 1, 0, 0, 1, 1, 1

Construct a matrix by aligning all distinct labels in the truth in one dimension and all distinct predicted labels in another one. It results in a $2\times2$, `square matrix`.

By placing the distinct `truth labels by column`, we have the following layout where each element cell represents one distinct value of the `(prediction, truth)` pair.

|                             | truth /reference /expected |                      |
|-----------------------:|:--------------------------:|:------------------:|
|              **prediction** |    truth: *0 / non-cat*    | truth: 1 / *non-cat* |
| **prediction**: 0 / non-cat | (prediction, truth):(0, 0) |        (0, 1)        |
|   **prediction***:* 1 / cat |           (1, 0)           |        (1, 1)        |

: The structure of confusion matrix with truth by rows

Alternatively, placing the distinct truth labels by row gives the transposed matrix. And each cell represents one distinct value of the `(truth, prediction)` pair.

|                         | [**prediction**]{.smallcaps} |         |
|------------------------:|:----------------------------:|:-------:|
| [**truth**]{.smallcaps} |         0 / non-cat          | 1 / cat |
|           *0 / non-cat* |  (truth, prediction):(0, 0)  | (0, 1)  |
|               *1 / cat* |            (1, 0)            | (1, 1)  |

: The structure of confusion matrix with truth by columns

::: {.callout-note appearance="simple"}
#### The dimensions of confusion matrix

A confusion matrix is a 2-dimensional, square matrix, having both dimensions being the number of dinstinct classes.
:::

### Frequency

Choosing the *truth-by-row* layout, we fill in each element with the number of occurrences for its associated value of *(prediction, truth)*. According the example set,

    id          : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
    prediction  : 1, 1, 1, 0, 0, 1, 0, 1, 1, 0
    truth       : 1, 0, 1, 1, 1, 0, 0, 1, 1, 1

The first row is $[1, 2]$ as $(0,0)$ occurs one time and $(0,1)$ twice. By completing other rows in the same way, the resulting matrix is

|             |  prediction   |           |
|------------:|:-------------:|:---------:|
|       truth | *0 / non-cat* | *1 / cat* |
| 0 / non-cat |       1       |     2     |
|     1 / cat |       3       |     4     |

: An example of confusion matrix

### Acronyms

The four counts in a binary confusion matrix are termed as True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN).

Assume **label 1 for cat is positive** class and **label 0 is negative** class.

::: panel-tabset
#### TP (True Positive)

TP is the number of cases whose prediction = 1 and truth = 1.

When the classifier says the case is positive, the truth is positive too.

#### FP (False Positive)

FP is the number of cases whose prediction = 1 but truth = 0.

The classifier says the case is positive; however, the truth is negative.

#### TN (True Negative)

TN is the number of cases whose prediction = 0 and truth = 0.

When the classifier says the case is negative, the truth is negative too.

#### FN (False Negative)

FN is the number of cases whose prediction = 0 but truth = 1.

The classifier says the case is negative; however, the truth is positive
:::

Put the name of each count into the previous matrix and we have

|              |   prediction   |               |
|-------------:|:--------------:|:-------------:|
|        truth | *0 / negative* | *1 /positive* |
| 0 / negative |     TN = 1     |    FP = 2     |
| 1 / positive |     FN = 3     |    TP = 4     |

: An example of confusion matrix

### Accuracy

Regardless of the matrix layout, the same observations can be derived from the matrix.

-   Of the 10 pictures, the classifier says that there were 6 positive (1/cat) and 4 negative (0/non-cat) cases;

-   Of the 7 actual positive cases, the classifier says 4 correctly as positive and 3 falsely as negative.

-   Similarly, of the 3 actual negative cases, 2 were falsely predicted as positive.

-   **All correct predictions are located in the diagonal of the matrix, so it is easy to visually inspect the matrix for errors, as they will be represented by any non-zero values outside the diagonal.**

::: column-margin
As the diagonal elements have *the same value for both truth and prediction*, they represent all the correct predictions.
:::

::: callout-tip
The *primary/principal diagonal* of a confusion matrix from top left to bottom right shows the correct predictions for each distinct class. The values on other positions are wrong predictions.

In other words, a good classifier should have a confusion matrix with large numbers diagonally and small (ideally zero) numbers off-diagonally.
:::

Taking as the overall accuracy, the ratio between the sum of the diagonal elements and the sum of the matrix, we have

$$
accuracy = \frac{1+4}{10}= 0.5
$$

### Visualization

A confusion matrix is often visualized in a heatmap by mapping each element to a specific color corresponding to its value. A heatmap is a data visualization type where the individual values contained in a matrix through variations in coloring.

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
# make two lists for expected/true and predicted/estimate target/label values
expected = [1,0,1,1,1,0,0,1,1,1]
predicted = [1,1,1,0,0,1,0,1,1,0]
# Create confusion matrix
cmat = confusion_matrix(expected, predicted)
# Visualize confusion matrix
# The viridis scale is good for colour blindness
fig, ax = plt.subplots()
img = ax.imshow(cmat, interpolation='nearest', cmap=plt.cm.get_cmap("viridis"))
ax.set_title('Confusion matrix')
#plt.title('Confusion matrix')
fig.colorbar(img)
ticks = np.arange(2)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_ylabel('True/Expected labels')
ax.set_xlabel('Predicted labels')
ax.minorticks_on()
ax.xaxis.set_minor_locator(AutoMinorLocator(2))
ax.yaxis.set_minor_locator(AutoMinorLocator(2))
ax.grid(linestyle = '--', which='minor')
plt.show()
```

## Exercises

Assume a binary classifier with the two labels `yes` and `no`. Given the following truth and prediction data.

    truth: "yes" "yes" "yes" "no"  "no"  "no"  "no"  "no"  "no"  "yes"
    prediction: "no"  "yes" "no"  "yes" "yes" "no"  "no"  "yes" "yes" "no"

Do the following:

1.  Construct a confusion matrix by hand.
2.  Calculate TP, FP, TN and FN.
3.  Calculate accuracy.

## Confusion Matrix in R

### Load libraries

```{r}
#| results: hide
#| warning: false
library(kableExtra)
library(tidyverse)
```

### Load truth & prediction

Combine the truth and prediction variables as two columns in a 2D object *pair*, which is of type `tibble`.

```{r}
#| eval: true
truth <- c(1,0,1,1,1,0,0,1,1,1)
prediction <- c(1,1,1,0,0,1,0,1,1,0)
pair <- tibble(truth, prediction)
pair
```

```{r include=FALSE,eval=FALSE}
pair |>
  kable(caption = "The truth and prediction data") |> 
  kable_classic(full_width = F)
```

### Calculate TP,FP,TN,FN

::: panel-tabset
#### TP (True Positive)

The cases having truth = 1 and prediction = 1

```{r eval=TRUE}
pair |>
  filter(truth==1 & prediction==1)  
```

#### FP (False Positive)

The number of cases whose truth = 0 and prediction = 1

```{r eval=TRUE}
pair |>
  filter(truth==0 & prediction==1) 
```

#### TN (True Negative)

The number of cases whose truth = 0 and prediction = 0

```{r eval=TRUE}
pair |>
  filter(truth==0 & prediction==0) 
```

#### FN (False Negative)

The number of cases whose truth = 1 and prediction = 0

```{r eval=TRUE}
pair |> 
  filter(truth==1 & prediction==0) 
```
:::

### table

Call R's built-in function `table`. There are two control options in the table function:

-   place the truth by rows
-   place the truth by columns

```{r eval=TRUE}
# truth by rows
cmat <- table(
  factor(truth, levels = c(0,1)),
  factor(prediction, levels = c(0,1)), 
  # dimension names
  dnn = c("truth","prediction"))
print(cmat)
```

```{r eval=TRUE}
# truth by columns
cmat <- table(
  factor(prediction, levels = c(0,1)), 
  factor(truth, levels = c(0,1)),
  # dimension names
  dnn = c("prediction","truth"))
print(cmat)
```

## R Caret Library

```{r}
#| results: hold
#| warning: false
library(kableExtra)
library(tidyverse)
library(caret)
truth <- c(1,0,1,1,1,0,0,1,1,1)
prediction <- c(1,1,1,0,0,1,0,1,1,0)
confusionMatrix(
  factor(prediction, levels = c(0,1)), 
  factor(truth, levels = c(0,1)),
  positive = "1",
  mode="everything"
  )
```

::: column-margin
::: {.callout-note appearance="minimal"}
#### The reference parameter in caret.confusionMatrix

In the *confusionMatrix* function from the *caret* library, the parameter's name for the True labels is *reference*.
:::
:::
