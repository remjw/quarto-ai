---
title: Confusion Matrix
---


A confusion matrix is a contingency table which counts each combinations of true labels and predicted labels. This name is used because the matrix makes it easy to visualize if the model confused or mislabeled two or more classes.

Confusion matrix is also referred to as **cross table** in statistics.

Assume a binary classifier has been trained to assign an integer label 0 or 1 for each case/data point.

In the example case, we **assume label 1 is positive class and label 0 is negative class**.

::: callout-note
Be aware that the class labels for positive and negative classes are user defined. Even though the labels can be arbitrary, their names are expected to indicate the meaning of two distinct classes.

For instance, if a classifier is purposed to identify cats, the positive class is cat and everything else should be in the negative class. The two labels can be the string values `cat` and `non-cat`.

Some of machine learning libraries requires integer labels. `cat` and `non-cat` must be converted to integer 1 and 0. Most modern libraries offer the user option to specify the positive and negative class.
:::

The `truth` variable stores the true labels of ten data points. The predictions from the classifier are stored in the `prediction` variable.

```{r}
#| echo: false
#| results: hold
truth <- c(1,0,1,1,1,0,0,1,1,1)
prediction <- c(1,1,1,0,0,1,0,1,1,0)
cat('truth: ', truth)
cat("\n")
cat('prediction: ', prediction)
```

With each column corresponding to one class in the truth, the confusion matrix is expected to be:

```{r}
#| echo: false
#| warning: false
library(caret)
cm <- confusionMatrix(
    factor(prediction, levels = c(1,0)),
  factor(truth, levels = c(1,0))
  )
cm$table
```

We first learn how a confusion matrix is built from scratch in the R language without using any external library. Then discuss the relevant performance metrics for evaluating a classifier.

Then we will take a look at the confusion_matrix and classification_report classes from *Python's* **sklearn.metrics** module.


## Build Confusion Matrix in R

### Load libraries

```{r}
#| results: hide
#| warning: false
library(kableExtra)
library(tidyverse)
```

### Load truth & prediction

Combine the truth and prediction variables as two columns in a 2D object *pair*, which is of type `tibble`.

```{r pair}
#| eval: true
truth <- c(1,0,1,1,1,0,0,1,1,1)
prediction <- c(1,1,1,0,0,1,0,1,1,0)
pair <- tibble(truth, prediction)
pair
```

```{r include=FALSE,eval=FALSE}
pair %>% kable(caption = "The truth and prediction data") %>% kable_classic(full_width = F)
```

### Calculate TP,FP,TN,FN

::: panel-tabset
**Assume label 1 is positive class and label 0 is negative class**. Calculate four counts: TP, FP, TN and FN.

#### TP (True Positive)

The case count whose truth = 1 and prediction = 1

```{r eval=TRUE}
pair %>% 
  filter(truth==1 & prediction==1) %>%
  summarise(TP=n()) 
```

#### FP (False Positive)

The number of cases whose truth = 0 and prediction = 1

```{r eval=TRUE}
pair %>% 
  filter(truth==0 & prediction==1) %>%
  summarise(FP=n()) 
```

#### TN (True Negative)

The number of cases whose truth = 0 and prediction = 0

```{r eval=TRUE}
pair %>% 
  filter(truth==0 & prediction==0) %>%
  summarise(TN=n()) 
```

#### FN (False Negative)

The number of cases whose truth = 1 and prediction = 0

```{r eval=TRUE}
pair %>% 
  filter(truth==1 & prediction==0) %>%
  summarise(FN=n())
```
:::


## Build confusion matrix

Call R's built-in function `table`. There are two control options in the table function:

- place the truth by rows
- place the truth by columns

```{r eval=TRUE}
# truth by rows
cmat <- table(
  factor(truth, levels = c(1,0)),
  factor(prediction, levels = c(1,0)), 
  # dimension names
  dnn = c("truth","prediction"))
print(cmat)
```

```{r eval=TRUE}
# truth by columns
cmat <- table(
  factor(prediction, levels = c(1,0)), 
  factor(truth, levels = c(1,0)),
  # dimension names
  dnn = c("prediction","truth"))
print(cmat)
```

Regardless of the layout, the same observations are derived from both. Of 10 case samples,

-   The classifier model predicted that there were 6 positive (1) and 4 negative (0) cases;

-   And of the 7 true positive (1) cases, the model predicted 4 as 1 and 3 as 0.

-   Similarly, of the 3 actual negative (0) cases, 2 were predicted as 1.

-   **All correct guesses are located in the diagonal of the matrix, so it's easy to visually inspect the matrix for errors, as they will be represented by any non-zero values outside the diagonal.**

