---
title: K-Fold Cross-Validation
format: 
  html: 
    code-fold: true 
    code-tools: true
    code-link: true
#execute:
#  echo: false 
#  cache: true
---

::: callout-important
#### K-fold cross-validation

Read Section 15.3.2. Use as a reference, [The sklearn's Cross-validation page](https://scikit-learn.org/stable/modules/cross_validation.html)

**K-fold cross-validation** is a model evaluation method for overcoming model **overfitting** issue from a **small-sized sample dataset**.

The method is used in choosing one learning model among several candidates. The method allows you to use all the data for both training and testing, and it makes good use of every sample in a small data set.
:::

## Three Types of Data Set

-   training set
-   test set
-   validation set

```{mermaid}
graph TD
    A[Load Data] -->|Holdout| B(Split: train 70%, test 30%) 
    B -->BB[Configure estimator/model]--> C[KFold with Training set]
    C -->|modeling 1| D[score 1] --> H[Take average as overall performance score]
    C -->|modeling 2| E[score 2]--> H
    C -->|...| G[...]--> H
    C -->|modeling K| F[score K]--> H
```

::: callout-important
-   Split the entire data into the training set and test set. Take the training set into the validation stage. The K-fold cross validation method splits the training set into K folds and output average performance metric(s).

-   Randomizing data samples by Shuffling as the samples might be ordered or grouped.
:::

## How it Works

Say you have a training set of one hundred or no more than one thousand samples. To apply *K-fold cross-validation*,

1.  Choose an integer value for K, the number of folds, often set to 5 or 10.
2.  Shuffle the data
3.  Split the data into `K equally-sized (and stratified)` **folds/subsets**.
4.  Repeat modeling for **K times** by rotating the validation set over every subset.
5.  The overall score is taken as the average performance of K resulting estimators.

```{mermaid}
gantt
title 3-fold cross validation with 9 samples
dateFormat  DD
axisFormat  %d
section modeling-1
Fold-1 for validation   : 01, 3d
section modeling-2
Fold-2 for validation   :04, 3d
section modeling-3
Fold-3 for validation   :07, 3d
```

## Two Stages

**sklearn.model_selection** module provides the functions for folding and validating.

::: panel-tabset

### Folding

Random split and stratified split

#### *KFold.split* function

First study a simple example, which shows how to perform data partition and configure the parameters for the following:

-   Split 9 data values to 3 folds
-   Access via the indexes, both training set and test set in each fold

```{python}
# Learn sklearn.KFold.split function
import numpy as np
from sklearn.model_selection import KFold

# data samples
data = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

# configure a new instance of KFold with specific setting
kfold = KFold(n_splits=3, shuffle=True, random_state=1)

# run split & print the splits
for train, test in kfold.split(data):
  print(f'TEST: {data[test]}, TRAIN: {data[train]}')
```

#### Split Dataset

To split a dataset having multiple features,

```{python}
# sklearn.KFold.split 
import numpy as np
from sklearn.model_selection import KFold
np.random.seed(99) # for reproducibility of result
labels = [0, 1]

# a dataset of 2 features x1, x2 & 1 label y
y = np.random.choice(labels, size=9, replace=True)
x1 = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
x2 = np.random.sample(9)
# stack x1 and x2 to X
X = np.stack((x1, x2), axis=-1)
print(f'X = \n {X}')

# configure a new instance of KFold with specific setting
kfold = KFold(n_splits=3, shuffle=True, random_state=1)

# run split & print the data indexes for train and test
for train_index, test_index in kfold.split(X):
  print(f'X_test index: {test_index}, X_train index: {train_index}')
```

#### *StratifiedKFold.split*

A **stratified K-fold partition** attempts to preserve the percentage of data samples for each class. The folds have the similar class distribution.

Taking the following X and y as an example,

```{python}
import numpy as np
from sklearn.model_selection import KFold

# A dataset of 2 features x1, x2 & 1 label y
N = 100
np.random.seed(99)
labels = [0, 1, 2, 3, 4]

y = np.random.choice(labels, size=N, replace=True)
x1 = np.random.default_rng(seed=12).random((N,))
x2 = x1 + y
X = np.stack((x1, x2), axis=-1)
# class distribution
print(f'''
Percentage of label 0: {round(sum(y==0)/len(y),5)}
Percentage of label 1: {round(sum(y==1)/len(y),5)}
Percentage of label 2: {round(sum(y==2)/len(y),5)}
Percentage of label 3: {round(sum(y==3)/len(y),5)}
Percentage of label 4: {round(sum(y==4)/len(y),5)}
''')
```

The stratified k-folds

> StratifiedKFold.split(X, y)

tries to maintain the same distribution in each fold:

```{python}
# 9-fold
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)

for train_index, test_index in skf.split(X, y):
  print(f'\ntraining set: size = {len(train_index)}')
  for i in range(0,5):
    print(f'P_{i} = {round(sum(y[train_index]==i)/len(train_index),5)}', end=' ')
  
  print(f'\ntest set: size = {len(test_index)}')
  for i in range(0,5):
    print(f'P_{i} = {round(sum(y[test_index]==i)/len(test_index),5)}', end=' ')
```


### Validating

```{mermaid}
graph TD
    A[Load Data] -->|Holdout| B(Split: train 70%, test 30%) 
    B -->BB[Configure estimator/model]--> C[KFold with Training set]
    C -->|modeling 1| D[score 1] --> H[Take average as overall performance score]
    C -->|modeling 2| E[score 2]--> H
    C -->|...| G[...]--> H
    C -->|modeling K| F[score K]--> H
```

```{python}
# auto-folding & manual cross-validating
import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

# configuration
np.random.seed(99)
labels = [0, 1, 2, 3, 4]
N = 100
N_folds = 3
holdout = 0.3
# A dataset of 2 features x1, x2 & 1 label y
y = np.random.choice(labels, size=N, replace=True)
x1 = np.random.default_rng(seed=12).random((N,))
x2 = x1 + y
X = np.stack((x1, x2), axis=-1)
# Holdout 70% for training, 30% for test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=holdout, shuffle=True, random_state=0)

# Take training set to folding
skf = StratifiedKFold(n_splits=N_folds, shuffle=True, random_state=1)
scores = []

for train_index, val_index in skf.split(X_train, y_train):
        # modeling 
        print(f'-- Modeling ')
        train = X_train[train_index]
        valid = X_train[val_index]
        output_train = y_train[train_index]
        output_valid = y_train[val_index]
        # train model
        model = LogisticRegression(random_state=0).fit(train, output_train)
        # predict the validation set
        prediction = model.predict(valid)
        print(f'''Validation fold: {len(valid)}
        Prediction: {prediction}
        truth: {output_valid}''')
        # confusion matrix
        cmat = confusion_matrix(y_true=output_valid, y_pred=prediction, labels=[0,1,2,3,4]) 
        print(cmat)
        # score the model
        score = model.score(valid, output_valid)
        # append model score
        scores.append(score)

print(f'\nscores = {scores}')
print(f'Mean-Accuracy: {sum(scores) / len(scores)}')

```


:::


## The *cross_validate* Function

Alternatively, **sklearn.model_selection.cross_validate** function wraps the entire process of folding and cross validation.

To run the function, configure the required parameters for the `estimator object`, `X_train`, `y_train`, `cv` (number of folds or an iterable from a fold function). The default of `scoring` option is accuracy for classification and error for regression.

::: panel-tabset

### Example 1

Run the *3-fold cross validation* to evaluate the *Logistic Regression estimator* with a 2-class problem using an artificial dataset of 100 samples by 2 features.

```{python}
# auto-folding & auto-cross-validating
import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate
from sklearn.linear_model import LogisticRegression

# configuration
np.random.seed(99) 
labels = [0, 1]
N = 100
N_folds = 3
holdout = 0.3 # ratio 
# A dataset of 2 features x1, x2 & 1 label y
y = np.random.choice(labels, size=N, replace=True)
x1 = np.array([0.1, 0.2, 0.3, 0.4, 0.5] * 20)
x2 = np.random.sample(N)
X = np.stack((x1, x2), axis=-1)
# Holdout 70% for training, 30% for test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=holdout, shuffle=True, random_state=0)
# configure a model
model = LogisticRegression(random_state=0)
# K-Fold CV using default score
scores = cross_validate(model, X_train, y_train, cv=N_folds, return_train_score=True, return_estimator=True)
print(scores)

print(f'Mean-Accuracy: {scores["test_score"].mean()}')
```

### Example 2

Generate a random 2-class classification problem. Run the *5-fold cross validation* to evaluate the *Logistic Regression estimator*.

```{python}
# Generate a random n-class classification problem
import numpy as np
from sklearn.datasets import make_classification
# n_samples 1000, n_features 20, n_classes=2, 
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, n_informative=10, random_state=0)
X = X.astype(np.float32)
y = y.astype(np.uint8)

N_folds = 5
holdout = 0.3 
# Holdout 70% for training, 30% for test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=holdout, shuffle=True, random_state=0)
# configure a model
model = LogisticRegression(random_state=0)
# K-Fold CV using default score accuracy
scores = cross_validate(model, X_train, y_train, cv=N_folds, return_train_score=True, return_estimator=True)
print(scores)
print(f'Mean-Accuracy: {scores["test_score"].mean()}')

```

### Example 3

Generate a random regression problem. Run the *5-fold cross validation* to evaluate the *Linear Regression estimator*.

The score is the coefficient of determination $R^2$, defined as $1 - \frac{u}{v}$, where

$$
u = \sum(y_{true} - y_{pred})^2, 
v = \sum(y_{true} - mean(y_{true}))^2
$$

The best $R^2$ score is the highest value, being 1.

```{python}
# Generate a random n-class classification problem
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
# n_samples 1000, n_features 20, n_classes=2, 
X, y = make_regression(n_samples=100, n_features=100, n_informative=10, random_state=0)
# X = X.astype(np.float32)


N_folds = 5
holdout = 0.3 
# Holdout 70% for training, 30% for test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=holdout, shuffle=True, random_state=0)
# configure a model
model = LinearRegression()
# K-Fold CV using default score 
scores = cross_validate(model, X_train, y_train, cv=N_folds, return_train_score=True, return_estimator=True)
print(scores)
print(f'Mean-score: {scores["test_score"].mean()}')

```

### Example 4

The MNIST database is a large database of handwritten digits that is commonly used for training various image processing and machine learning systems. The data contains 60,000 training images and 10,000 testing images.

Run the *3-fold cross validation* to evaluate an *SGD classifier*. We decrease the size of the training set from 60,000 to 100 for less execution time.

```{python}
# auto-cv
import numpy as np
from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate
from sklearn.datasets import fetch_openml
from sklearn.linear_model import SGDClassifier

mnist_df = fetch_openml('mnist_784', version=1) 
X, y = mnist_df.data, mnist_df.target
# print(X.shape, y.shape, X.columns)
y = y.astype(np.uint8) #string to int
# Take first N samples as training data
N = 1000 #60000
X_train, X_test, y_train, y_test = X.iloc[:N], X.iloc[N:], y[:N], y[N:]
y_train = (y_train == 0) # digit 0 vs. other digits
y_test = (y_test == 0)
# configure a model
model = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)
N_folds = 3 # quick test by 3 folds instead of 10 folds
# K-Fold CV using default score accuracy
scores = cross_validate(model, X_train, y_train, cv=N_folds, return_train_score=True, return_estimator=True)
print(scores)
print(f'Mean-Accuracy: {scores["test_score"].mean()}')
```


:::

## Exercise

Write Python code which runs the *10-fold cross validation* to evaluate the *LinearRegression estimator* for predicting house prices using the *California Housing data*.

```{python}
# Perform *10-fold cross validation* to evaluate the *LinearRegression estimator* for predicting house prices using the *California Housing data*.

# load california housing data
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()
print(housing.DESCR)
# get X and y
X, y = housing.data, housing.target
# Take a look at the values of sample at index 1
print(X[1,:], y[1])
```

```{python}
#| eval: false
#| echo: false
N_folds = 10
holdout = 0.3 
# Holdout 70% for training, 30% for test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=holdout, shuffle=True, random_state=0)
# configure a model
model = LinearRegression()
# K-Fold CV using default score
scores = cross_validate(model, X_train, y_train, cv=N_folds, return_train_score=True, return_estimator=True)
print(scores)
print(f'Mean-score: {scores["test_score"].mean()}')

```

```{python}
#| eval: false
#| echo: false
## mnist with 10-fold Cross Validation
import sklearn
assert sklearn.__version__ >= "0.24"
print(sklearn.__version__)
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_val_score

mnist_df = fetch_openml('mnist_784', version=1) 
X, y = mnist_df.data, mnist_df.target
#print(X.shape, y.shape, X.columns)

y = y.astype(np.uint8) #string to int
X_train, X_test, y_train, y_test = X.iloc[:60000], X.iloc[60000:], y[:60000], y[60000:]
y_train = (y_train == 0) # digit 0 detector
y_test = (y_test == 0)

model = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)
num_folds = 3 # To have a quick test, we do 3 folds instead of 10 folds
skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

accuracy_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')
print("Accuracy: " + str(round(100*accuracy_scores.mean(), 2)) + "%")

precision_scores = cross_val_score(model, X_train, y_train, scoring='precision_weighted', cv=skf)
print("Precision: " + str(round(100*precision_scores.mean(), 2)) + "%")

recall_scores = cross_val_score(model, X_train, y_train, scoring='recall_weighted', cv=skf)
print("Recall: " + str(round(100*recall_scores.mean(), 2)) + "%")

f1_scores = cross_val_score(model, X_train, y_train, scoring='f1_weighted', cv=skf)
print("F1: " + str(round(100*f1_scores.mean(), 2)) + "%")


#Shuffle the digits data (if data is ordered or grouped) and split the whole set into 10 equal-size folds. Run 10-fold cross-validation

 
'''KFold Cross-Validation'''
# 1. use the KFold Object. shuffle data and create 10 folds
from sklearn.model_selection import KFold
kfold = KFold(n_splits=10, random_state=11, shuffle=True)

# 2. use the Function cross_val_score
# train knn with feature and target data
from sklearn.model_selection import cross_val_score
scores = cross_val_score(estimator=knn, X=digits.data, y=digits.target, cv=kfold)
# 3. Print 10 scores
print(scores)

print(f'Mean accuracy of 5-nn estimators is {scores.mean():.3%}.')
```
